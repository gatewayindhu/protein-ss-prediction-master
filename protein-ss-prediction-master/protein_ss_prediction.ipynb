{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/perought/protein-ss-prediction/blob/master/protein_ss_prediction.ipynb\">\n",
    "<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"colab\" style=\"float: left;\">\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vxVkxexg1jC5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from torchsummary import summary\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "plt.style.use('dark_background') # dark_background, default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a6F2qrpfiw-w"
   },
   "outputs": [],
   "source": [
    "!git clone --quiet https://github.com/upsheroes3/dataset.git > /dev/null # dataset 1\n",
    "# https://www.kaggle.com/alfrandom/protein-secondary-structure\n",
    "%cp -r drive/MyDrive/datasets/protein-secondary-structure protein-secondary-structure # dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2zE8RTXxeJDG"
   },
   "outputs": [],
   "source": [
    "def get_aa_onehot_map(all_aa):\n",
    "    aa_map = {aa: i for i, aa in enumerate(all_aa)}\n",
    "    aa_onehot = np.zeros((20, 20))\n",
    "    aa_onehot[np.arange(20), np.array(range(20))] = 1\n",
    "    return {aa: aa_onehot[i] for i, aa in enumerate(all_aa)}\n",
    "  \n",
    "def get_ss_map(total_ss):\n",
    "    if total_ss == 3:\n",
    "        ss_map = {\"E\": 0, \"H\": 1, \"T\": 2}\n",
    "        ss_map_r = {0: \"E\", 1: \"H\", 2: \"T\"}\n",
    "        return ss_map, ss_map_r\n",
    "    elif total_ss == 8:\n",
    "        ss_map = {\"E\": 0, \"H\": 1, \"T\": 2, \"C\": 3, \"S\": 4, \"B\": 5, \"G\": 6, \"I\": 7 }\n",
    "        ss_map_r = {0: \"E\", 1: \"H\", 2: \"T\", 3: \"C\", 4: \"S\", 5: \"B\", 6: \"G\", 7: \"I\"}\n",
    "        return ss_map, ss_map_r  \n",
    "\n",
    "def get_sequences(filename, total_ss, csv_file=False):\n",
    "    all_aa = ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y']\n",
    "    aa_onehot_map = get_aa_onehot_map(all_aa)\n",
    "    ss_map, ss_map_r = get_ss_map(total_ss)\n",
    "    names = []\n",
    "    seq_list = []\n",
    "    ss_list = []\n",
    "    if not csv_file:\n",
    "        with open(filename, \"r\") as f:\n",
    "            data = f.readlines() # fasta format\n",
    "\n",
    "        for i in range(0, len(data), 3):\n",
    "            seq = data[i + 1].rstrip().upper()\n",
    "            ss = data[i + 2].rstrip().upper()\n",
    "            clean_seq = \"\"\n",
    "            clean_ss = \"\"\n",
    "            for j in range(len(seq)):\n",
    "                if seq[j] in all_aa and ss[j] in ss_map:\n",
    "                    clean_seq += seq[j]\n",
    "                    clean_ss += ss[j]\n",
    "            \n",
    "            names.append(data[i].rstrip())\n",
    "            seq_list.append(clean_seq)\n",
    "            ss_list.append(clean_ss)\n",
    "\n",
    "        actual_seq = \"\".join(seq_list)\n",
    "        actual_ss = \"\".join(ss_list)\n",
    "        return all_aa, aa_onehot_map, ss_map, ss_map_r, actual_seq, actual_ss\n",
    "    \n",
    "    else:\n",
    "        pdb_df = pd.read_csv(filename)\n",
    "        seq = \"\".join(pdb_df[\"seq\"].tolist())\n",
    "        if total_ss == 3:\n",
    "            ss = \"\".join(pdb_df[\"sst3\"].tolist())\n",
    "        else:\n",
    "            ss = \"\".join(pdb_df[\"sst8\"].tolist())\n",
    "\n",
    "        actual_seq = \"\"\n",
    "        actual_ss = \"\"\n",
    "        for j in range(len(seq)):\n",
    "            if seq[j] in all_aa and ss[j] in ss_map:\n",
    "                actual_seq += seq[j]\n",
    "                actual_ss += ss[j]\n",
    "\n",
    "        return all_aa, aa_onehot_map, ss_map, ss_map_r, actual_seq, actual_ss\n",
    "\n",
    "total_ss = 3\n",
    "all_aa, aa_onehot_map, ss_map, ss_map_r, actual_seq, actual_ss = get_sequences(\"dataset/data_dump.txt\", total_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "id": "qTg8dj_OeJAz",
    "outputId": "c60fe8f3-7520-4c3c-eefb-0dec05b18768"
   },
   "outputs": [],
   "source": [
    "# dataset 1\n",
    "ss_count = {}\n",
    "for ss in actual_ss:\n",
    "    if ss not in ss_count:\n",
    "        ss_count[ss] = 0\n",
    "    else:\n",
    "        ss_count[ss] += 1\n",
    "print([round(c / sum(ss_count.values()), 4) for c in ss_count.values()])\n",
    "mc = np.array(sorted(ss_count.items(), key=lambda x: x[1], reverse=True))\n",
    "_, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.bar(mc[:, 0], np.array(mc[:, 1], dtype=int))\n",
    "plt.title(\"Secondary structure distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "id": "lx1IDIdoeI_C",
    "outputId": "3089d0bf-0a66-41c8-ec77-a326907248a3"
   },
   "outputs": [],
   "source": [
    "aa_count = {}\n",
    "for aa in actual_seq:\n",
    "    if aa not in aa_count:\n",
    "        aa_count[aa] = 0\n",
    "    else:\n",
    "        aa_count[aa] += 1\n",
    "mc = np.array(sorted(aa_count.items(), key=lambda x: x[1], reverse=True))\n",
    "_, ax = plt.subplots(figsize=(10, 5))\n",
    "plt.bar(mc[:, 0], np.array(mc[:, 1], dtype=int))\n",
    "plt.ylim(ymin=100)\n",
    "plt.title(\"Amino acid distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "id": "o7ckHzOV1fq8",
    "outputId": "4e729bce-327d-4f59-db6e-ec776a54f14d"
   },
   "outputs": [],
   "source": [
    "# dataset 2\n",
    "ss_count = {}\n",
    "for ss in actual_ss:\n",
    "    if ss not in ss_count:\n",
    "        ss_count[ss] = 0\n",
    "    else:\n",
    "        ss_count[ss] += 1\n",
    "print([round(c / sum(ss_count.values()), 4) for c in ss_count.values()])\n",
    "mc = np.array(sorted(ss_count.items(), key=lambda x: x[1], reverse=True))\n",
    "_, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.bar(mc[:, 0], np.array(mc[:, 1], dtype=int))\n",
    "plt.title(\"Secondary structure distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "id": "3lD7CYgH1ygC",
    "outputId": "a8806c66-bbe0-40e2-a328-22994d123e8c"
   },
   "outputs": [],
   "source": [
    "aa_count = {}\n",
    "for aa in actual_seq:\n",
    "    if aa not in aa_count:\n",
    "        aa_count[aa] = 0\n",
    "    else:\n",
    "        aa_count[aa] += 1\n",
    "mc = np.array(sorted(aa_count.items(), key=lambda x: x[1], reverse=True))\n",
    "_, ax = plt.subplots(figsize=(10, 5))\n",
    "plt.bar(mc[:, 0], np.array(mc[:, 1], dtype=int))\n",
    "plt.ylim(ymin=100)\n",
    "plt.title(\"Amino acid distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2qbrgEcreI8m"
   },
   "outputs": [],
   "source": [
    "class ProteinDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, all_seq, all_ss, w_size, aa_onehot, ss_onehot_map):\n",
    "        self.all_seq = all_seq\n",
    "        self.all_ss = all_ss\n",
    "        self.w_size = w_size\n",
    "        self.aa_onehot = aa_onehot\n",
    "        self.ss_onehot_map = ss_onehot_map\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_seq) - self.w_size + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq_onehot = [self.aa_onehot[self.all_seq[aa]] for aa in range(idx, idx + self.w_size)]\n",
    "        ss_onehot = self.ss_onehot_map[self.all_ss[idx + int(self.w_size / 2)]]\n",
    "        return np.array(seq_onehot)[None], np.array(ss_onehot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/onehot.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/architecture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "4cvgSl9bnJ6J"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mNet\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, height, out_size):\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;28msuper\u001b[39m(Net, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, height, out_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.height = height\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.bn_2d_a = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.bn_2d_b = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "\n",
    "        \"\"\"\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=8,\n",
    "            hidden_size=64,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=8,\n",
    "            hidden_size=64,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=8,\n",
    "            hidden_size=64,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        \"\"\"\n",
    "\n",
    "        self.fc1 = nn.Linear(64 * int((self.height - 4) / 2) * 8, 128)\n",
    "        self.bn_1d_a = nn.BatchNorm1d(128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.bn_1d_b = nn.BatchNorm1d(64)\n",
    "        self.fc3 = nn.Linear(64, out_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn_2d_a(self.conv1(x)))\n",
    "        x = self.relu(self.bn_2d_b(self.conv2(x)))\n",
    "        \n",
    "        x = self.dropout1(self.pool(x))\n",
    "\n",
    "        # x, h_n = self.rnn(x.view(-1, int((self.height - 4) / 2), 8), None)\n",
    "        # x, h_n = self.gru(x.view(-1, int((self.height - 4) / 2), 8), None)\n",
    "        # x, (h_n, h_c) = self.lstm(x.view(-1, int((self.height - 4) / 2), 8), None)\n",
    "        # x = torch.flatten(x[:, -1, :], 1)\n",
    "        # x = x.view(128, -1)\n",
    "        \n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.relu(self.bn_1d_a(self.fc1(x)))\n",
    "        x = self.relu(self.bn_1d_b(self.fc2(x)))\n",
    "        \n",
    "        x = self.fc3(self.dropout2(x))\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wZ4oq6EhnJzM"
   },
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device, dtype=torch.float), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target.type(torch.LongTensor).cuda())\n",
    "        running_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 5000 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch + 1, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "    \n",
    "    return running_loss / batch_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PCr8TchQnJw9"
   },
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device, dtype=torch.float), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target.type(torch.LongTensor).cuda(), reduction='sum').item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    \n",
    "    return correct / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "62ug0IB7nJun",
    "outputId": "3020ef70-dd2b-4400-b8ea-1eb8d7820795"
   },
   "outputs": [],
   "source": [
    "# dataset 1\n",
    "w_size = 21\n",
    "batch_size = 128\n",
    "total_ss = 3\n",
    "\n",
    "all_aa, aa_onehot_map, ss_map, ss_map_r, actual_seq, actual_ss = get_sequences(\"dataset/data_dump.txt\", total_ss)\n",
    "seq_train, seq_test, ss_train, ss_test = train_test_split(actual_seq, actual_ss, test_size=0.2, shuffle=False)\n",
    "\n",
    "train_dataset = ProteinDataset(seq_train, ss_train, w_size, aa_onehot_map, ss_map)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_dataset = ProteinDataset(seq_test, ss_test, w_size, aa_onehot_map, ss_map)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Net(w_size, total_ss).to(device)\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "# optimizer = torch.optim.Adadelta(model.parameters(), lr=0.01)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n",
    "\n",
    "loss_values = []\n",
    "accuracy_values = []\n",
    "test(model, device, test_loader)\n",
    "for epoch in range(5):\n",
    "    train_loss = train(model, device, train_loader, optimizer, epoch)\n",
    "    loss_values.append(train_loss)\n",
    "\n",
    "    test_acc = test(model, device, test_loader)\n",
    "    accuracy_values.append(test_acc)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "torch.save(model.state_dict(), \"model_ss3.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "id": "mLF3WqXEeI6Q",
    "outputId": "e00718b5-58a4-46bf-9c5c-2958aad02402"
   },
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(np.arange(1, 6).astype(int), loss_values, label=\"train loss\")\n",
    "ax.plot(np.arange(1, 6).astype(int), accuracy_values, label=\"test accuracy\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss and Acc')\n",
    "plt.title('Train Loss and Test Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HGf3OC64yvae",
    "outputId": "67b94711-ed5a-411f-e3c4-ffd99eee65e9"
   },
   "outputs": [],
   "source": [
    "# dataset 2\n",
    "w_size = 21\n",
    "batch_size = 128\n",
    "total_ss = 8\n",
    "\n",
    "all_aa, aa_onehot_map, ss_map, ss_map_r, actual_seq, actual_ss = get_sequences(\"kaggle-pdb.csv\", total_ss, csv_file=True)\n",
    "seq_train, seq_test, ss_train, ss_test = train_test_split(actual_seq, actual_ss, test_size=0.2, shuffle=False)\n",
    "\n",
    "train_dataset = ProteinDataset(seq_train, ss_train, w_size, aa_onehot_map, ss_map)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_dataset = ProteinDataset(seq_test, ss_test, w_size, aa_onehot_map, ss_map)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Net(w_size, total_ss).to(device)\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "# optimizer = torch.optim.Adadelta(model.parameters(), lr=0.01)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n",
    "\n",
    "loss_values = []\n",
    "accuracy_values = []\n",
    "test(model, device, test_loader)\n",
    "for epoch in range(5):\n",
    "    train_loss = train(model, device, train_loader, optimizer, epoch)\n",
    "    loss_values.append(train_loss)\n",
    "\n",
    "    test_acc = test(model, device, test_loader)\n",
    "    accuracy_values.append(test_acc)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "torch.save(model.state_dict(), \"model_ss8.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "id": "7MgGTsTOzkRB",
    "outputId": "cd683929-03a4-4a5c-ad1c-b8eb3d33ce37"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m _, ax \u001b[38;5;241m=\u001b[39m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39msubplots(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[0;32m      2\u001b[0m ax\u001b[38;5;241m.\u001b[39mplot(np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m6\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m), loss_values, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain loss\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m ax\u001b[38;5;241m.\u001b[39mplot(np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m6\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m), accuracy_values, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "_, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(np.arange(1, 6).astype(int), loss_values, label=\"train loss\")\n",
    "ax.plot(np.arange(1, 6).astype(int), accuracy_values, label=\"test accuracy\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss and Acc')\n",
    "plt.title('Train Loss and Test Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P_J_QMZ4eI2m",
    "outputId": "866a8bb9-561e-4cc4-9e4e-e4db7f2db96f"
   },
   "outputs": [],
   "source": [
    "summary(model, input_size=(1, 21, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 375
    },
    "id": "wpPdEc6g5LK1",
    "outputId": "12eb51d2-82cd-47a9-aa73-18eb82195069"
   },
   "outputs": [],
   "source": [
    "w_size = 21\n",
    "batch_size = 512\n",
    "datasets = ProteinDataset(seq_train, ss_train, w_size, aa_onehot_map, ss_map)\n",
    "loader = torch.utils.data.DataLoader(datasets, batch_size=batch_size, shuffle=True)\n",
    "next_batch_0 = next(iter(loader))\n",
    "next_batch_1 = next(iter(loader))\n",
    "next_batch_2 = next(iter(loader))\n",
    "next_batch_3 = next(iter(loader))\n",
    "next_batch_4 = next(iter(loader))\n",
    "next_batch_5 = next(iter(loader))\n",
    "\n",
    "all_next_batch_x = torch.cat((\n",
    "    next_batch_0[0], next_batch_1[0], next_batch_2[0], \n",
    "    next_batch_3[0], next_batch_4[0], next_batch_5[0]\n",
    "))\n",
    "all_next_batch_y = torch.cat((\n",
    "    next_batch_0[1], next_batch_1[1], next_batch_2[1], \n",
    "    next_batch_3[1], next_batch_4[1], next_batch_5[1]\n",
    "))\n",
    "all_E_table = all_next_batch_x[all_next_batch_y == 0]\n",
    "all_H_table = all_next_batch_x[all_next_batch_y == 1]\n",
    "all_T_table = all_next_batch_x[all_next_batch_y == 2]\n",
    "print(\"E: {}, H: {}, T: {}\".format(len(all_E_table), len(all_H_table), len(all_T_table)))\n",
    "\n",
    "min_table = min(len(all_E_table), len(all_H_table), len(all_T_table))\n",
    "all_E_table = all_E_table[:min_table]\n",
    "all_H_table = all_H_table[:min_table]\n",
    "all_T_table = all_T_table[:min_table]\n",
    "all_E_summed = np.squeeze(all_E_table.sum(axis=0), axis=0)\n",
    "all_H_summed = np.squeeze(all_H_table.sum(axis=0), axis=0)\n",
    "all_T_summed = np.squeeze(all_T_table.sum(axis=0), axis=0)\n",
    "print(\"E: {}, H: {}, T: {}\".format(all_E_summed.max(), all_H_summed.max(), all_T_summed.max()))\n",
    "\n",
    "vmax = max(all_E_summed.max(), all_H_summed.max(), all_T_summed.max())\n",
    "vmax = min(all_E_summed.max(), all_H_summed.max(), all_T_summed.max())\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(40, 10)) # 10, 30\n",
    "\n",
    "sns.heatmap(all_E_summed, cmap=\"Greys\", xticklabels=all_aa, ax=axs[0], vmin=0, vmax=vmax)\n",
    "axs[0].set_title(\"Visualization of {} E Batchs\".format(len(all_E_table)), fontsize=18)\n",
    "axs[0].set_xlabel(\"Amino Acids\", fontsize=18)\n",
    "axs[0].set_ylabel(\"Window Index\", fontsize=18)\n",
    "axs[0].invert_yaxis()\n",
    "\n",
    "sns.heatmap(all_H_summed, cmap=\"Greys\", xticklabels=all_aa, ax=axs[1], vmin=0, vmax=vmax)\n",
    "axs[1].set_title(\"Visualization of {} H Batchs\".format(len(all_H_table)), fontsize=18)\n",
    "axs[1].set_xlabel(\"Amino Acids\", fontsize=18)\n",
    "axs[1].set_ylabel(\"Window Index\", fontsize=18)\n",
    "axs[1].invert_yaxis()\n",
    "\n",
    "sns.heatmap(all_T_summed, cmap=\"Greys\", xticklabels=all_aa, ax=axs[2], vmin=0, vmax=vmax)\n",
    "axs[2].set_title(\"Visualization of {} T Batchs\".format(len(all_T_table)), fontsize=18)\n",
    "axs[2].set_xlabel(\"Amino Acids\", fontsize=18)\n",
    "axs[2].set_ylabel(\"Window Index\", fontsize=18)\n",
    "axs[2].invert_yaxis()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 504
    },
    "id": "oqG7eZVl-x3q",
    "outputId": "988f1869-d1c7-4db0-a6cf-abb8550885c8"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ProteinDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m w_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m21\u001b[39m\n\u001b[0;32m      2\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m----> 3\u001b[0m datasets \u001b[38;5;241m=\u001b[39m \u001b[43mProteinDataset\u001b[49m(seq_train, ss_train, w_size, aa_onehot_map, ss_map)\n\u001b[0;32m      4\u001b[0m loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(datasets, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      5\u001b[0m temp_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(loader))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ProteinDataset' is not defined"
     ]
    }
   ],
   "source": [
    "w_size = 21\n",
    "batch_size = 1\n",
    "datasets = ProteinDataset(seq_train, ss_train, w_size, aa_onehot_map, ss_map)\n",
    "loader = torch.utils.data.DataLoader(datasets, batch_size=batch_size, shuffle=True)\n",
    "temp_batch = next(iter(loader))\n",
    "viz_batch = temp_batch[0]\n",
    "print(viz_batch.shape)\n",
    "summed = np.squeeze(viz_batch.sum(axis=0), axis=0)\n",
    "print(summed.shape)\n",
    "plt.figure(figsize=(10,7))\n",
    "ax = sns.heatmap(summed, cmap=\"Greys\", xticklabels=all_aa)\n",
    "plt.title(\"Visualization of {} Batch\".format(ss_map_r[temp_batch[1].item()]), fontsize=18)\n",
    "plt.xlabel(\"Amino Acids\", fontsize=18)\n",
    "plt.ylabel(\"Window Index\", fontsize=18)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EivIBjtzAbQ4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "MB9setTiL1W9",
    "EQMIRbG2ggx0",
    "d-ojSrXeL25x",
    "74jVB7fWRNUv",
    "k3Zb0yF0_1oB",
    "okYjBEaOhbyx",
    "YRYJSdjgiv7C"
   ],
   "name": "proteins.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
